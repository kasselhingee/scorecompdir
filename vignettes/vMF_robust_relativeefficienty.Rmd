---
title: "vMF Relative Efficiency for Robust"
author: "Kassel Hingee"
date: "30/06/2022"
output: html_document
---

Mardia et al 2016, Table 3, presents relative efficiency to the maximum likelihood estimator as a ratio of mean squared error.
This document is designed to generate a similar table for the robust (via Windham's method) hybrid estimator.

We'll also redo the relative efficiency estimates for the non-robust hybrid estimator to check results.

The estimate for the mean direction is the same in the hybrid and maximum likelihood. But the robust version will have *different* mean direction estimates due to the weights.

## Preparation
The package `CircStats` was used by Mardia et al. for simulation. 
This package appears to be only for circular (`p=2`) models, with data provided in radians.

```{r setup, include=FALSE}
library(cdabyppi)

p = 2
library(CircStats)
m <- c(1,1)/sqrt(2)
k <- 3
Y <- Directional::rvmf(10000, m, k) #just for development
```

### Maximum-Likelihood Estimator
The maximum-likelihood estimator is the mean direction, and the kappa
```{r mlefun}
mle_circ <- function(Y){
  stopifnot(ncol(Y) == 2)
  Y_rad <- atan(Y[,2]/Y[,1]) #CircStats::rvm(10000, atan(1), 3)
  m_rad <- circ.mean(Y_rad)
  m <- c(cos(m_rad), sin(m_rad))
  k <- est.kappa(Y_rad)
  return(list(m = m, k = k))
}
if(!isTRUE(all.equal(mle_circ(Y), list(m=m, k=k), tolerance = 1E-1))){rm("mle_circ")} #mle_circ doesn't work

mle_Dir <- function(Y){
  fit <- Directional::vmf.mle(Y, fast = TRUE)
  return(list(
    m = fit$mu,
    k = fit$kappa
  ))
}
if(!isTRUE(all.equal(mle_Dir(Y), list(m=m, k=k), tolerance = 1E-1))){rm("mle_Dir")}

mle <- mle_Dir
```

### Function: Simulate then Estimate
```{r sim_estimate}
simnest <- function(n, m, k){
  Y <- Directional::rvmf(n, m, k)
  est <- list()
  est$ml <- mle(Y)
  est$hyb <- vMF(Y, method = "Mardia")[c("m", "k")]
  est$hyb_rob <- vMF(Y, method = "Mardia", cW = 0.1)[c("m", "k")]
  est$hyb_robsm <- vMF(Y, method = "Mardia_robustsm", cW = 0.1)[c("m", "k")]
  est$fsm <- vMF(Y, method = "smfull")[c("m", "k")]
  est$fsm_rob <- vMF(Y, method = "smfull", cW = 0.1)[c("m", "k")]
  return(est)
}
err_k <- function(out, truek){(vapply(out, "[[", "k", FUN.VALUE = 0.1) - truek)}
# out <- replicate(reps, simnest(1000, m, k), simplify = TRUE)
# mse <- rowMeans(apply(out, MARGIN = 2, err_k, truek = k)^2)
out2mse_k <- function(out, k){rowMeans(apply(out, MARGIN = 2, err_k, truek = k)^2)}
out2mse_m <- function(out, m){
  se_m <- apply(out, MARGIN = c(1,2), FUN = function(est){sum((est[[1]][["m"]] - m)^2)})
  mse_m <- rowMeans(se_m)
  return(mse_m)
}
runexperiment <- function(reps, n, m, k){
  stopifnot(reps > 1) #cos simplify below will be all strange when reps = 1
  out <- replicate(reps, simnest(n, m, k), simplify = TRUE)
  mse_k <- out2mse_k(out, k)
  mse_m <- out2mse_m(out, m)
  return(list(
    mse_k = mse_k,
    mse_m = mse_m,
    ests = out
  ))
}
```

### Model Parameters for Simulation
```{r truepars}
m <- c(1,1)/sqrt(2)
ks <- seq(0.5, 10, by = 0.5)
```

## The n=100 case
```{r n100, warning=FALSE}
n100 <- pbapply::pblapply(ks, function(k){runexperiment(1000000, 100, m, k)})
names(n100) <- ks
save(n100, file = "n100.rds")
load("./n100.rds")
mse_k <- t(simplify2array(lapply(n100, "[[", "mse_k")))
mse_m <- t(simplify2array(lapply(n100, "[[", "mse_m")))
print("relative MSE for k")
1/(mse_k[,-1]/mse_k[, "ml"])
print("relative MSE for m")
1/(mse_m[,-1]/mse_m[, "ml"])
```

```{r n100plot}
library(ggplot2)
library(dplyr)
as_tibble(1/(mse_k[,-1]/mse_k[, "ml"]), rownames = "k") %>%
  dplyr::mutate(k = as.numeric(k)) %>%
  tidyr::pivot_longer(-k, names_to = "Estimator", values_to = "relMSE") %>%
  dplyr::mutate(Method = gsub("_.*", "", Estimator)) %>%
  dplyr::mutate(Robust = case_when(
    grepl("_robsm", Estimator) ~ "k only",
    grepl("_rob$", Estimator) ~ "all",
    TRUE ~ "none"
  )) %>%
  ggplot() +
  geom_line(aes(x = k, y = relMSE, col = Method, lty = Robust)) +
  geom_point(aes(x = k, y = relMSE, col = Method)) +
  ggtitle("Relative MSE for Kappa")

as_tibble(1/(mse_m[,-1]/mse_m[, "ml"]), rownames = "k") %>%
  dplyr::mutate(k = as.numeric(k)) %>%
  tidyr::pivot_longer(-k, names_to = "Estimator", values_to = "relMSE") %>%
  dplyr::mutate(Method = gsub("_.*", "", Estimator)) %>%
  dplyr::mutate(Robust = case_when(
    grepl("_robsm", Estimator) ~ "k only",
    grepl("_rob$", Estimator) ~ "all",
    TRUE ~ "none"
  )) %>%
  ggplot() +
  geom_line(aes(x = k, y = relMSE, col = Method, lty = Robust)) +
  geom_point(aes(x = k, y = relMSE, col = Method)) +
  ggtitle("Relative MSE for mu")
```

It looks like the robust versions have much lower efficiency than the non-robust versions, and I suspect it is because of the estimation of the mean direction.
Robust fitting of *just* kappa is more efficient, but only a little bit.
